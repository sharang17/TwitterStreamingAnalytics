for(i in n-k){
if(all(x[i:i+2]==1)){runs<-c(runs,i)}
}
return(runs)
}
findruns(x,2)
findruns<-function(x,k){
n<-length(x)
for(n in n-k){
findruns<-function(x,k)
findruns<-function(x,k){
n<-length(x)
runs<-NULL
for(i in (n-k)){
if(all(x[i:(i+k)]==1)){runs<-c(runs,i)}
}
return(runs)
}
}
}
findruns(x,2)
findruns<-function(x,k){
n<-length(x)
runs<-NULL
for(i in (n-k+1)){
if(all(x[i:(i+k-1)]==1))runs<-c(runs,i)
}
return(runs)
}
findruns(x)
findruns(x,2)
predict<-function(x,k){}
predict<-function(x,k){
n<-length(x)
k2=k/2
pred<-vector(length=n-k)
install.packages('Hmisc')
install.packages("Hmisc")
z12<-function(x)return(x,x^2)
z12(12)
z12<-function(x)return(c(x,x^2))
z12(25)
x<-c[1:10]
x<-c(1:10
x
x<-c(1:10)
x
z12(x)
y<-z12()
y<-z12(x)
matrix(y)
matrix(y,ncol=2)
matrix(y,nrow=2)
sapply(1:10,z12)
v<-c(1:10)
v
v<-v[v>4]
v
v<-c(1:10)
v<-v[v*2>10]
v
v<-v[v*v>v+v]
v
v[v>8]<-0
v
library(arules)
data(BJSales)
library(datasets)
data(BJRules)
data(BJsales)
summary(BJsales)
data(WWWUsage)
'data(esoph)
library(datasets)
data(ChickWeight)
View(ChickWeight)
data(Titanic)
View(Titanic)
data(USPersonalExpenditure)
View(UsPersonalExpenditure)
View(USPersonalExpenditure)
library(arules)
data(AdultUCI)
View(AdultUCI)
summary(AdultUCI)
data(UCBAdmissions)
View(UCBAdmissions)
x<-matrix(1:6,2,3)
x
x<-list(a<-list(1,2,3),b<-list(4,5,6))
x[1,3]
x(1,2)
x(c(1,2))
x[c(1,2)]
x[[c(1,2)]]
x
x$a
x
x<--4L
type(x)
x
class(x)
x<-(4,"a",TRUE)
x<-c(4,"a",TRUE)
class(x)
x<-(1,3,5)
x<-c(1,3,5)
y<-c(4,5,6)
z=rbind(x,y)
z
class(z)
x<-c(1,"a","b")
x[[2]]
x<-1:4
y<-2:3
x+y
z<-x+y
class(z)
hw1_data <- read.csv("D:/College/Computers/Coursera/R Programming/Week1/QUiz/rprog_data_quiz1_data/hw1_data.csv")
View(hw1_data)
a<-hw1_data
a[2,]
a[nrows=2]
b<-a(nrows=2)
a[[2,]]
a[2,2]
a(2,)
a[1:2]
a[2:1]
length(a)
a(4,1)
a[4,1]
z<-is.na(a[,2])
z
a[,2]
z<-is.na(a[,2])
z
z<-is.na(a[,1])
z
x<-mean(a[,1])
x
x<-mean(a[,1],na.rm=FALSE)
x
x<-mean(a[,1],na.rm=TRUE)
x
z<-list(a[,1],a[,2])
z
z<-matrix(a[,1],a[,2])
z
z<-data.frame(a[,1]>31)
z
class(a)
z<-a[which(Ozone>31 & Temp>90)]
z<-a[which(a$Ozone>31 & a$Temp>90)]
a$Ozone
a[47,1]
z<-subset(a,Ozone>31 & Temp>90)
z
m<-mean(z[,2])
m
m<-subset(a,Month==6)
m
v<-mean(m[,4],na.rm=TRUE)
v
v<-subset(a,Month==5)
v
v<-max(v[,1])
v
v<-max(v[,1],na.rm=TRUE)
v<-max([,1])
v<-max((,1)
v<-max((,1)
cube<-function(x,n){
source("http:''bioconductor.org/bioCLite.R")
source("http://bioconductor.org/bioCLite.R")
source("https://bioconductor.org/bioCLite.R")
source("https://bioconductor.org/biocLite.R")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
created=h5createFile("example.h5")
created=h5createGroup("example.h5","foo")
created=h5createGroup("example.h5","baa")
created=h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
library(httr)
oauth_endpoints("github")
myapp<-oauth_app("datacrunch",key="CLI myapp <- oauth_app("github", "ClientID", "ClientSecret")
myapp <- oauth_app("datacrunch", "ClientID", "ClientSecret")
github_token <- oauth2.0_token(oauth_endpoints("datacrunch"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
q()
library(httre)
library(httr)
oauth_endpoints("github")
github_token<-oauth2.0_app("github","ClientID","ClientSecret")
github_token<-oauth_app("github","ClientID","ClientSecret")
myapp<-oauth_app("github","ClientID","ClientSecret")
github_token<-oauth2.0_token(oauth_endpoints("github"),myapp)
myapp<-oauth2.0_app("datacrunch","ClientID","ClientSecret")
myapp<-oauth_app("datacrunch","ClientID","ClientSecret")
github_token<-oauth2.0_token(oauth_endpoints("github"),myapp)
q()
x<-1:4
lapply(x,mean)
x<-list(a=1:4)
lapply(x,mean)
lapply(x,runif)
help(runif)
x<-1:4
x<-1:4
lapply(x,runif)
x<-matrix(rnorm(200),20,10)
x
apply(x,2,mean)
x<-matrix(rnorm(200),20,10)
apply(x,2,mean)
apply(x,1,sum)
a<-array(rnorm(2*2*10),c(2,2,10))
a
apply(a,1,mean)
a<-array(rnorm(40),c(2,2,10))
a
apply(a,c(1,2),mean)
rowMeans(a,dims = 2)
array(1:3,c(2,4,10))
a<-array(1:3,c(2,4,10))
apply(a,1,sum)
a<-array(1:3,c(2,4,2))
a
apply(a,1,sum)
apply(a,c(1,2),sum)
rnorm(10)
a<-c(rnorm(10),runif(10),rnorm(10,1))
f<-gl(3,10)
f
a
tapply(a,f,mean)
mean(rnorm(10))
tapply(a,f,mean,simplify=FALSE)
library(datasets)
head(airquality)
split(airquality,airquality$Month)
s<-split(airquality,airquality$Month)
lapply(s,function(x) colMeans(x([,c("Solar.R","Ozone","Wind")])))
lapply(s,function(x) colMeans(x[,c("Solar.R","Ozone","Wind")]))
lapply(s,function(x) colMeans(x[,c("Solar.R","Ozone","Wind")]))
lapply(s,function(x) colMeans(x[,c("Solar.R","Ozone","Wind")],na.rm=TRUE))
View(airquality)
s<-split(airquality,list(airquality$Month,airquality$Day))
str(s)
head(s)
str(s,drop=TRUE)
sapply(s,mean)
sapply(s,mean,na.rm=TRUE)
lapply(s,mean,na.rm=TRUE)
a<-lapply(s,mean,na.rm=TRUE)
head(a)
data(iris)
View(iris)
a<-tapply(iris,iris$Sepal.length,mean)
split(iris,iris$Species)
subset(iris,species==virginica)
subset(iris,Species==virginica)
subset(iris,Species=="virginica")
s<-subset(iris,Species=="virginica")
mean(s$Sepal.Length)
colMeans(iris)
apply(iris[,1:4],2,mean)
data(mtcars)
View(mtcars)
?mtcars
sapply(mtcars,cyl,mean)
sapply(split(mtcars$mpg,mtcars$cyl),mean)
cyl4<-subset(mtcars,cyl==4)
cyl8<-subset(mtcars,cyl==8)
hp4<-mean(cyl4$hp)
hp8<-mean(cyl8$hp)
abs(hp4-hp8)
debug(ls)
ls
install.packages("swirl")
library(swirl)
swirl()
5+7
x<-5+7
x
y<-x-3
y
z<-c(1.1,9,3.14)
?c
z
c(z,555)
c(z,555,z)
z*2+100
my_sqrt<-sqrt(z-1)
my_sqrt
my_div<-z/my_sqrt
my_div
c(1,2,3,4)+c(0,10)
c(1,2,3,4)+c(0,10,100)
c(1,2,3,4)+c(0,10,100)
z*2+100
(z*2+100)
z*2_100
z*2+100
z*2+1000
my_div
getwd()
ls()
x<-9
ls
ls()
list.files()
?list.files
args(list.files())
args(list.files
)
old.dir<-getwd()
getwd()
getwd()
exit()
old.dir<-getwd()
dir.create("testdir")
ls()
list.dirs()
setwd("testdir")
file.create("mytest.R")
ls.files()
ls()
list.files()
file.exists("mytest.R")
file.info("mytest.R")
file.rename("mytest.R","mytest2.R")
file.copy("mytest2.R",mytest3.R)
file.copy("mytest2.R","mytest3.R")
?file.path
file.path("/mytest.R")
file.path("/mytest3.R")
file.path("mytest3.R")
file.path("folder1","folder2")
?dir.create
dir.create("testdir2")
info()
dir.create(file.path("testdir2","testdir3"),recrusive=TRUE)
dir.create(file.path("testdir2","testdir3"),recursive=TRUE)
unlink("testdir2")
unlink("testdir2",recursive=TRUE)
setwd("old.dir")
old.dir
setwd(old.dir)
unlink("testdir",recursive=TRUE)
1:20
pi:10
15:1
?:
?":"
seq(1,20)
seq(0,10,by=0.5)
my_seq<-seq(5,10,length=30)
length(my_seq)
1:length(my_seq)
seq(along.with=my_seq)
seq_along(my_seq)
sqirl()
swirl()
rep(0,times=40)
rep(c(0,1,2),times=10)
rep(c(0,1,2),each=10)
num_vect<-c(0.5,55,-10,6)
tf<-num_vect<1
tf
num_vect>-=6
num_vect>=6
my_char<-c("My","name","is")
my_char
paste(my_char,collapse=" ")
my_name<-c(my_char,"Sharang")
my_name
paste(my_name,collapse=" ")
paste("Hello","world",sep=" ")
paste("Hello","world!",sep=" ")
paste(1:3,c("X","Y","Z"),sep="")
paste(LETTERS,1:4,sep="-")
info()
bye()
install.packages("shiny")
install.packages("plotly")
library(shiny)
runExample("01_hello")
library(nlme)
library(lattice)
xyplot(weight~time|Diet,Bodyweight)
xyplot(weight~Time|Diet,Bodyweight)
xyplot(weight~Time|Diet,BodyWeight)
View(BodyWeight)
library(ggplot2)
qplot(Wind,Ozone,data=airquality,facets=.~factor(Month))
airquality<-transform(airquality,Month=factor(Month))
qplot(Wind,Ozone,data=airquality,facets=.~Month
)
g<-ggplot(movies,aes(votes,rating))
print(g)
qplot(votes,rating,data=movies) + geom_smooth()
qplot(votes,rating,data=movies,smooth="loess")
qplot(votes,rating,data=movies) + geom_smooth()
?splom()
?print.trellis()
?trellis.par.set()
?lines()
?points()
?axis()
library(twitteR)
APIKey<-"x1YcIk1ax3S5XIGFqF248DCsH"
APISecret<-" ibo2wrzSet0ggSlDU9kPWE4W9chNPEBGq1tyUO9zpN5KuPW1Qz"
AccessKey<-"417969461-Ne627qiJefn490UPIM5K0QsDssiJlHD9dwsW3ysv"
AccessSecret<-"A992UzVXNKifgayqGyDRncxLeA53oneoZpmcvwmEVmWEF"
setup_twitter_oauth(APIKey,APISecrety,AccessKey,AccessSecret)
setup_twitter_oauth(APIKey,APISecret,AccessKey,AccessSecret)
setup_twitter_oauth(APIKey,APISecret)
setup_twitter_oauth(APIKey,APISecret,AccessKey,AccessSecret)
APISecret<-"ibo2wrzSet0ggSlDU9kPWE4W9chNPEBGq1tyUO9zpN5KuPW1Qz"
setup_twitter_oauth(APIKey,APISecret,AccessKey,AccessSecret)
search_twitter_and_store("pvrcinemas","tweets")
?register_db_backend
tweets<-searchTwitter("pvrcinemas",n=150)
head(tweets)
tweets<-as.data.frame(tweets)
View(tweets)
tweets<-do.call("rbind",lapply(tweets,as.data.frame(tweets)))
tweets<-do.call("rbind",lapply(tweets,as.data.frame))
View(tweets)
write.csv(tweets)
write.csv(tweets,file="C:/Users/SHARANG/Documents/GitHub/prophesee/tweets.csv")
library(RMySql)
library(RMySQL)
install.packages("RSQLite")
library(RSQLite)
dim(tweets)
library(tm)
myCorpus<-Corpus(VectorSource(tweets$text))
myCorpus
myCorpus<-tm_map(myCorpus,tolower)
myCorpus<-tm_map(myCorpus,removePunctuation)
myCorpus<-tm_map(myCorpus,removeNumbers)
stopWords<-c(stopwords('english'))
myCorpus<-tm_map(myCorpus,removeWords,stopWords)
install.packages("SnowballC")
getwd()
setwd("C:/Users/SHARANG/Documents/GitHub")
list.files
list.files)
list.files()
setwd("C:/Users/SHARANG/Documents/GitHub/college_tweets_analytics")
library(shiny)
runApp("Collegetweetsanalytics")
runApp("Collegetweetanalytics")
setwd("C:/Users/SHARANG/Documents/GitHub")
library(streamR)
twitter_url = "http://search.twitter.com/search.atom?"
getwd()
setwd("C:/Users/SHARANG/Documents/GitHub/Twitter-Analytics")
source("test_stream.R")
install.packages("XML")
source("test_stream.R")
source("streamanalytics.R")
source("streamanalytics.R")
source("streamanalytics.R")
source("streamanalytics.R")
tweets<-parseTweets("current_tweets.json")
View(tweets)
View(tweets$text)
tweets.text<-sapply(X = tweets$text,FUN = function(x) as.character(tweets$text))
View(tweets.text)
length(tweets.text)
chars<-sapply(tweets.text,nchar)
chars
summart(chars)
summary(chars)
words_list = strsplit(tweets.text, " ")
words_per_tweet = sapply(words_list, length)
words_per_tweet
summary(words_list)
barplot(table(words_per_tweet), border=NA,
main="Distribution of words per tweet", cex.main=1)
View(tweet.text)
View(tweets)
tweet.text<-tweets$text
length(tweets.text)
ncol(tweet.text)
nrow(tweet.text)
chars_per_tweet = sapply(tweet.text, nchar)
summary(chars_per_tweet)
words_list = strsplit(tweet.text, " ")
words_per_tweet = sapply(words_list, length)
summary(words_per_tweet)
barplot(table(words_per_tweet), border=NA,
main="Distribution of words per tweet", cex.main=1)
wsize_per_tweet = sapply(words_list, function(x) mean(nchar(x)))
barplot(table(round(wsize_per_tweet)), border=NA,
xlab = "word length in number of characters",
main="Distribution of words length per tweet", cex.main=1)
uniq_words_per_tweet = sapply(words_list, function(x) length(unique(x)))
barplot(table(uniq_words_per_tweet), border=NA,
main="Distribution of unique words per tweet", cex.main=1)
hash_per_tweet = sapply(words_list, function(x) length(grep("#", x)))
table(hash_per_tweet)
prop.table(table(hash_per_tweet))
barplot(table(hash_per_tweet), border=NA,
main="Distribution of hashtags", cex.main=1)
source("streamanalytics.R")
source("streamanalytics.R")
